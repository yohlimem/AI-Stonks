{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mplfinance as mpf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"AAPL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahli\\AppData\\Local\\Temp\\ipykernel_32544\\590745289.py:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_hour_data = combined_data.groupby(\"day_index\").apply(lambda x: x.iloc[-1])\n"
     ]
    }
   ],
   "source": [
    "def prepare_daily_data(stock_name, period=\"5d\", interval=\"1m\"):\n",
    "    recent_data = yf.download(stock_name, period=period, interval=interval)\n",
    "    recent_data = recent_data.drop(columns=[\"Adj Close\", \"Volume\"])\n",
    "\n",
    "    segment = 390 if interval == \"1m\" else 7\n",
    "\n",
    "    for i in range(1, segment):\n",
    "        recent_data[f\"Open t - {i}\"] = recent_data[\"Open\"].shift(i)\n",
    "        recent_data[f\"Close t - {i}\"] = recent_data[\"Close\"].shift(i)\n",
    "        recent_data[f\"High t - {i}\"] = recent_data[\"High\"].shift(i)\n",
    "        recent_data[f\"Low t - {i}\"] = recent_data[\"Low\"].shift(i)\n",
    "\n",
    "    recent_data.dropna(inplace=True)\n",
    "    first_timestamp = recent_data.index[0]\n",
    "\n",
    "    if first_timestamp.time() != pd.Timestamp(\"09:30:00\").time():\n",
    "        index = recent_data.index.get_loc(\n",
    "            recent_data[\n",
    "                recent_data.index.time == pd.Timestamp(\"09:30:00\").time()\n",
    "            ].index[0]\n",
    "        )\n",
    "        recent_data = recent_data.iloc[index:]\n",
    "\n",
    "    # Split data into daily segments\n",
    "    recent_data[\"day_index\"] = recent_data.index.date\n",
    "    recent_data[\"day_index\"], _ = pd.factorize(recent_data[\"day_index\"])\n",
    "    daily_data = [group for _, group in recent_data.groupby(\"day_index\")]\n",
    "\n",
    "    # Add movement column to each daily DataFrame\n",
    "    for day_data in daily_data:\n",
    "        day_data[\"movement\"] = (\n",
    "            day_data[\"Close\"].iloc[-1] > day_data[\"Open\"].iloc[0]\n",
    "        ).astype(int)\n",
    "\n",
    "    day_data[\"movement\"] = day_data[\"movement\"].shift(-1)\n",
    "\n",
    "    combined_data = pd.concat(daily_data)\n",
    "\n",
    "    last_hour_data = combined_data.groupby(\"day_index\").apply(lambda x: x.iloc[-1])\n",
    "    last_hour_data.dropna(inplace=True)\n",
    "\n",
    "    return last_hour_data\n",
    "\n",
    "daily_data = prepare_daily_data(stock_name, period=\"1y\", interval=\"1h\")\n",
    "\n",
    "print(daily_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 29])\n",
      "torch.Size([50, 29])\n",
      "tensor([[175.7300, 176.0600, 175.1900,  ..., 177.3300, 175.8450,  46.0000],\n",
      "        [173.9100, 174.2300, 173.6100,  ..., 171.9700, 170.1200,  55.0000],\n",
      "        [210.5500, 210.9900, 209.4100,  ..., 212.9000, 206.4100, 246.0000],\n",
      "        ...,\n",
      "        [189.0650, 189.6600, 188.9800,  ..., 190.0000, 188.0900,  77.0000],\n",
      "        [169.9900, 170.2000, 169.8300,  ..., 171.9700, 169.9000, 139.0000],\n",
      "        [181.2100, 182.3900, 181.2050,  ..., 182.8888, 181.5550, 130.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "X = daily_data.drop(columns=[\"movement\"])\n",
    "y = daily_data[\"movement\"]\n",
    "\n",
    "X_tensor = torch.from_numpy(X.values)\n",
    "# X_tensor.requires_grad=True\n",
    "y_tensor = torch.from_numpy(y.values)\n",
    "# y_tensor.requires_grad = True\n",
    "\n",
    "# print(X)\n",
    "\n",
    "# print(split_data(X_tensor, interval=\"1h\").shape)\n",
    "\n",
    "#print(split_data(X_tensor, interval=\"1h\").shape)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tensor,y_tensor,test_size=0.2)\n",
    "X_train = X_train.to(torch.float32).to(device)\n",
    "# X_time_step = [i for i in range(len(X_train))]\n",
    "X_test= X_test.to(torch.float32).to(device)\n",
    "y_train = y_train.to(torch.float32).to(device)\n",
    "y_test =y_test.to(torch.float32).to(device)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31.9592],\n",
       "        [31.1276],\n",
       "        [37.0827],\n",
       "        [33.5037],\n",
       "        [33.4757],\n",
       "        [31.9352],\n",
       "        [40.1227],\n",
       "        [34.2507],\n",
       "        [31.8910],\n",
       "        [34.3560],\n",
       "        [34.7974],\n",
       "        [34.7077],\n",
       "        [35.9986],\n",
       "        [34.2234],\n",
       "        [37.4235],\n",
       "        [33.3580],\n",
       "        [32.2581],\n",
       "        [32.2848],\n",
       "        [33.1019],\n",
       "        [34.1744],\n",
       "        [32.3549],\n",
       "        [34.0168],\n",
       "        [33.6309],\n",
       "        [29.6586],\n",
       "        [37.8666],\n",
       "        [29.6378],\n",
       "        [39.8426],\n",
       "        [38.4181],\n",
       "        [36.8757],\n",
       "        [34.5062],\n",
       "        [37.4741],\n",
       "        [34.7263],\n",
       "        [32.3813],\n",
       "        [32.7713],\n",
       "        [30.2866],\n",
       "        [30.8962],\n",
       "        [31.1081],\n",
       "        [29.8704],\n",
       "        [34.2181],\n",
       "        [30.9963],\n",
       "        [37.6095],\n",
       "        [32.0275],\n",
       "        [31.8576],\n",
       "        [29.7581],\n",
       "        [37.8298],\n",
       "        [31.3573],\n",
       "        [29.8526],\n",
       "        [31.9943],\n",
       "        [32.2013],\n",
       "        [31.1357],\n",
       "        [34.4630],\n",
       "        [30.0084],\n",
       "        [32.4934],\n",
       "        [33.4060],\n",
       "        [32.2207],\n",
       "        [34.4089],\n",
       "        [34.0310],\n",
       "        [30.6945],\n",
       "        [37.5604],\n",
       "        [32.4186],\n",
       "        [29.6010],\n",
       "        [33.4144],\n",
       "        [30.2309],\n",
       "        [32.0100],\n",
       "        [32.6194],\n",
       "        [32.2412],\n",
       "        [32.3815],\n",
       "        [36.8063],\n",
       "        [30.3949],\n",
       "        [32.2281],\n",
       "        [32.9463],\n",
       "        [33.5637],\n",
       "        [32.9866],\n",
       "        [29.9246],\n",
       "        [31.4606],\n",
       "        [32.4491],\n",
       "        [29.1563],\n",
       "        [37.2792],\n",
       "        [34.8003],\n",
       "        [29.8638],\n",
       "        [33.1209],\n",
       "        [38.6773],\n",
       "        [38.4055],\n",
       "        [30.2803],\n",
       "        [38.0932],\n",
       "        [30.6398],\n",
       "        [36.9495],\n",
       "        [29.8579],\n",
       "        [30.2510],\n",
       "        [39.6082],\n",
       "        [32.9883],\n",
       "        [32.9124],\n",
       "        [34.4274],\n",
       "        [33.2332],\n",
       "        [34.8240],\n",
       "        [32.0263],\n",
       "        [30.4149],\n",
       "        [40.1917],\n",
       "        [31.7695],\n",
       "        [29.8212],\n",
       "        [30.3439],\n",
       "        [34.6064],\n",
       "        [33.4889],\n",
       "        [32.0019],\n",
       "        [30.7927],\n",
       "        [33.5876],\n",
       "        [33.6218],\n",
       "        [38.0156],\n",
       "        [31.8112],\n",
       "        [34.6018],\n",
       "        [31.5416],\n",
       "        [34.2050],\n",
       "        [33.9920],\n",
       "        [29.6616],\n",
       "        [31.8883],\n",
       "        [33.6307],\n",
       "        [41.0859],\n",
       "        [34.6849],\n",
       "        [30.3077],\n",
       "        [35.0750],\n",
       "        [32.2001],\n",
       "        [30.8969],\n",
       "        [33.7766],\n",
       "        [30.3299],\n",
       "        [37.0229],\n",
       "        [38.3861],\n",
       "        [31.7009],\n",
       "        [32.2486],\n",
       "        [29.9913],\n",
       "        [32.8400],\n",
       "        [30.9388],\n",
       "        [30.0961],\n",
       "        [30.7597],\n",
       "        [30.1330],\n",
       "        [34.4591],\n",
       "        [34.7376],\n",
       "        [33.0988],\n",
       "        [38.4350],\n",
       "        [29.9794],\n",
       "        [32.1264],\n",
       "        [32.3872],\n",
       "        [30.7104],\n",
       "        [32.4156],\n",
       "        [34.3046],\n",
       "        [33.6103],\n",
       "        [33.9709],\n",
       "        [34.4724],\n",
       "        [33.0279],\n",
       "        [31.9008],\n",
       "        [32.8589],\n",
       "        [33.3677],\n",
       "        [33.2797],\n",
       "        [31.2569],\n",
       "        [39.4498],\n",
       "        [34.1964],\n",
       "        [35.4618],\n",
       "        [32.9307],\n",
       "        [31.2139],\n",
       "        [32.3770],\n",
       "        [31.4453],\n",
       "        [40.7454],\n",
       "        [32.4112],\n",
       "        [34.5928],\n",
       "        [34.0881],\n",
       "        [32.9471],\n",
       "        [32.5408],\n",
       "        [36.5479],\n",
       "        [33.7759],\n",
       "        [32.9114],\n",
       "        [34.8686],\n",
       "        [33.3544],\n",
       "        [29.2676],\n",
       "        [40.5739],\n",
       "        [32.2102],\n",
       "        [32.4150],\n",
       "        [34.7883],\n",
       "        [32.3139],\n",
       "        [39.4985],\n",
       "        [32.3417],\n",
       "        [39.5226],\n",
       "        [34.5853],\n",
       "        [36.8540],\n",
       "        [33.5219],\n",
       "        [29.7487],\n",
       "        [29.8328],\n",
       "        [34.9836],\n",
       "        [31.2157],\n",
       "        [32.4294],\n",
       "        [31.5960],\n",
       "        [32.9593],\n",
       "        [32.5538],\n",
       "        [35.5748],\n",
       "        [34.3316],\n",
       "        [34.1740],\n",
       "        [31.3731],\n",
       "        [34.5125],\n",
       "        [33.9422],\n",
       "        [30.0301],\n",
       "        [32.0595]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class LSTMPredictor(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, n_layers=2):\n",
    "#         super(LSTMPredictor, self).__init__()\n",
    "\n",
    "#         self.ltsm = nn.LSTM(\n",
    "#             input_size=input_size,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=n_layers,\n",
    "#             batch_first=True,\n",
    "#         )\n",
    "#     def forward(self, sequences):\n",
    "#         lstm_out, (hn, cn) = self.ltsm(sequences)\n",
    "#         return lstm_out\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(29, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1),\n",
    "    # nn.Sigmoid(),\n",
    ").to(device)\n",
    "\n",
    "model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=29, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCEWithLogitsLoss()\n",
      "epoch: 0 loss = 15.75103759765625, test loss = 3.0682640075683594\n",
      "epoch: 100 loss = 0.6498472094535828, test loss = 0.6815155148506165\n",
      "epoch: 200 loss = 0.5818423628807068, test loss = 0.6387889385223389\n",
      "epoch: 300 loss = 0.5735960602760315, test loss = 0.5643951892852783\n",
      "epoch: 400 loss = 0.5103606581687927, test loss = 0.5348231196403503\n",
      "epoch: 500 loss = 0.4925963878631592, test loss = 0.5338248610496521\n",
      "epoch: 600 loss = 0.5267376899719238, test loss = 0.6108317971229553\n",
      "epoch: 700 loss = 0.4463370442390442, test loss = 0.47252362966537476\n",
      "epoch: 800 loss = 0.4354885220527649, test loss = 0.5241014957427979\n",
      "epoch: 900 loss = 0.5646586418151855, test loss = 0.6581045389175415\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "print(loss_fn)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    y_logits = model(X_train).squeeze()\n",
    "    y_preds = torch.round(torch.sigmoid(y_logits)).float()\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_test_logits = model(X_test).squeeze()\n",
    "        y_test_preds = torch.round(torch.sigmoid(y_test_logits)).float()\n",
    "\n",
    "        # print(y_test_preds.shape)\n",
    "        # print(y_test.shape)\n",
    "        test_loss = loss_fn(y_test_logits, y_test)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch: {epoch} loss = {loss}, test loss = {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1.], grad_fn=<RoundBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.round(torch.sigmoid(model(X_train).squeeze())).float())\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_forcast_data(stock_name, period=\"5d\", interval=\"1m\"):\n",
    "    recent_data = yf.download(stock_name, period=period, interval=interval)\n",
    "    recent_data = recent_data.drop(columns=[\"Adj Close\", \"Volume\"])\n",
    "\n",
    "    segment = 390 if interval == \"1m\" else 7\n",
    "\n",
    "    for i in range(1, segment):\n",
    "        recent_data[f\"Open t - {i}\"] = recent_data[\"Open\"].shift(i)\n",
    "        recent_data[f\"Close t - {i}\"] = recent_data[\"Close\"].shift(i)\n",
    "        recent_data[f\"High t - {i}\"] = recent_data[\"High\"].shift(i)\n",
    "        recent_data[f\"Low t - {i}\"] = recent_data[\"Low\"].shift(i)\n",
    "\n",
    "    recent_data.dropna(inplace=True)\n",
    "    first_timestamp = recent_data.index[0]\n",
    "\n",
    "    if first_timestamp.time() != pd.Timestamp(\"09:30:00\").time():\n",
    "        index = recent_data.index.get_loc(\n",
    "            recent_data[\n",
    "                recent_data.index.time == pd.Timestamp(\"09:30:00\").time()\n",
    "            ].index[0]\n",
    "        )\n",
    "        recent_data = recent_data.iloc[index:]\n",
    "\n",
    "    # Split data into daily segments\n",
    "    recent_data[\"day_index\"] = recent_data.index.date\n",
    "    recent_data[\"day_index\"], _ = pd.factorize(recent_data[\"day_index\"])\n",
    "    daily_data = [group for _, group in recent_data.groupby(\"day_index\")]\n",
    "\n",
    "    # Add movement column to each daily DataFrame\n",
    "\n",
    "\n",
    "    combined_data = pd.concat(daily_data)\n",
    "\n",
    "    last_hour_data = combined_data.groupby(\"day_index\").apply(lambda x: x.iloc[-1])\n",
    "    last_hour_data.dropna(inplace=True)\n",
    "\n",
    "    return last_hour_data.iloc[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0', grad_fn=<RoundBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\yahli\\AppData\\Local\\Temp\\ipykernel_32544\\309001893.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  last_hour_data = combined_data.groupby(\"day_index\").apply(lambda x: x.iloc[-1])\n"
     ]
    }
   ],
   "source": [
    "pred_data = prepare_forcast_data(stock_name, period=\"1y\", interval=\"1h\")\n",
    "input_tensor = torch.from_numpy(pred_data.values)\n",
    "input_tensor =input_tensor.to(torch.float32).to(device)\n",
    "raw_answer = model(input_tensor).squeeze()\n",
    "answer = torch.round(torch.sigmoid(raw_answer)).float()\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_loop_data(stock_name, period=\"5d\", interval=\"1m\",back_time=5, answers =[]):\n",
    "\n",
    "    recent_data = yf.download(stock_name, period=period, interval=interval)\n",
    "\n",
    "    data_200 = yf.download(stock_name, period=period, interval=interval).tail(len(recent_data) + 201)\n",
    "    data_200 = data_200.drop(columns=[\"Adj Close\"])\n",
    "\n",
    "    data_200.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "    #recent_data = data_200.tail(len(recent_data))\n",
    "\n",
    "\n",
    "    recent_data = recent_data.dropna()\n",
    "    for i in range(back_time):\n",
    "        recent_data[f\"Open t - {i+1}\"] = recent_data[\"Open\"].shift((i+1))\n",
    "        recent_data[f\"Close t - {i+1}\"] = recent_data[\"Close\"].shift((i+1))\n",
    "        recent_data[f\"High t - {i+1}\"] = recent_data[\"High\"].shift((i+1))\n",
    "        recent_data[f\"Low t - {i+1}\"] = recent_data[\"Low\"].shift((i+1))\n",
    "    recent_data[\"movement\"] = (recent_data[\"Close\"] > recent_data[\"Open\"]).astype(int)\n",
    "\n",
    "    # recent_data = recent_data.tail(1)\n",
    "    # add_plot = mpf.make_addplot(recent_data[\"200_day\"], color=\"blue\", linestyle=\"--\")\n",
    "\n",
    "    return recent_data\n",
    "\n",
    "#recent_data = prepare_test_loop_data(stock_name,back_time=1,answers=[answer])\n",
    "#recent_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x42 and 29x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m check_y_tensor \u001b[38;5;241m=\u001b[39m check_y_tensor\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# print(check_data_last.to_markdown(), \"\\n\\n\\n\")\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m answer \u001b[38;5;241m=\u001b[39m model(check_X_tensor)\n\u001b[0;32m     35\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(answer, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x42 and 29x128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "answers = []\n",
    "forcast = 10\n",
    "check_data = prepare_test_loop_data(\n",
    "    interval=\"1m\", back_time=10, period=\"1d\", stock_name=stock_name\n",
    ")\n",
    "date_range = pd.date_range(\n",
    "    start=check_data.index[-1], periods=forcast, freq=\"1T\"\n",
    ")  # Generate a date range\n",
    "\n",
    "\n",
    "\n",
    "for i in range(forcast):\n",
    "    # for i in answers:\n",
    "\n",
    "    check_data_last = check_data.tail(1)\n",
    "    check_X = check_data_last.drop([\"Close\", \"Volume\", \"High\", \"Low\",\"movement\"], axis=1)\n",
    "    check_y = check_data_last[[\"movement\"]]\n",
    "\n",
    "    # print(check_data.tail(6).to_markdown())\n",
    "    # print(X.shape)\n",
    "\n",
    "    check_X_tensor = torch.from_numpy(check_X.values)\n",
    "    check_y_tensor = torch.from_numpy(check_y.values)\n",
    "\n",
    "    check_X_tensor = check_X_tensor.to(torch.float32).to(device)\n",
    "    check_y_tensor = check_y_tensor.to(torch.float32).to(device)\n",
    "\n",
    "    # print(check_data_last.to_markdown(), \"\\n\\n\\n\")\n",
    "\n",
    "    answer = model(check_X_tensor)\n",
    "    a = torch.softmax(answer, dim=1).argmax(dim=1)\n",
    "    print(a)\n",
    "\n",
    "    answer = answer.detach().cpu().numpy()\n",
    "\n",
    "    answer = np.append(answer[0],check_X[\"Close t - 1\"])\n",
    "    answers.append(answer)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Close\": [answers[-1][0]],\n",
    "            \"High\": [answers[-1][1]],\n",
    "            \"Low\": [answers[-1][2]],\n",
    "            \"Open\": [answers[-1][3]],\n",
    "            \"Adj Close\": [0],\n",
    "            \"Volume\": [0],\n",
    "        },\n",
    "        index=[date_range[i]],\n",
    "    )\n",
    "\n",
    "    check_data = check_data._append(df)\n",
    "    for i in range(10):\n",
    "        check_data[f\"Open t - {i+1}\"] = check_data[\"Open\"].shift((i + 1))\n",
    "        check_data[f\"Close t - {i+1}\"] = check_data[\"Close\"].shift((i + 1))\n",
    "        check_data[f\"High t - {i+1}\"] = check_data[\"High\"].shift((i + 1))\n",
    "        check_data[f\"Low t - {i+1}\"] = check_data[\"Low\"].shift((i + 1))\n",
    "    recent_data[\"movement\"] = (recent_data[\"Close\"] > recent_data[\"Open\"]).astype(int)\n",
    "\n",
    "    # check_data.dropna(inplace=True)\n",
    "# print(check_data.to_markdown())\n",
    "\n",
    "# print(check_data)\n",
    "# print(answers)\n",
    "\n",
    "# sns.catplot(answer[0][:],label=\"Predicted\")\n",
    "last_elements = [arr[0] for arr in answers]\n",
    "\n",
    "recent_data = prepare_data(stock_name, period=\"1d\", interval=\"1m\")\n",
    "# print(\"Converted index to datetime\")\n",
    "\n",
    "answers_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            recent_data,\n",
    "            columns=[\"Close\", \"High\", \"Low\", \"Open\",\"movement\"],\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            answers, columns=[\"movement\"], index=date_range\n",
    "        ),\n",
    "    ]\n",
    ").iloc[200:]\n",
    "\n",
    "# print(answers_df.tail(100))\n",
    "\n",
    "mpf.plot(\n",
    "    answers_df,\n",
    "    type=\"candle\",\n",
    "    style=\"charles\",\n",
    "    title=f\"{stock_name} Candlestick Chart\",\n",
    "    ylabel=\"Price\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 24\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Build model\n",
    "class stonks(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "             nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features), \n",
    "            nn.Sigmoid(),\n",
    "     \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "model = stonks(input_features=X_train.size()[1], \n",
    "                    output_features=1, \n",
    "                    hidden_units=8).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1976e-05],\n",
      "        [6.7180e-05],\n",
      "        [2.2683e-05],\n",
      "        [2.2327e-05]], grad_fn=<SliceBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Make prediction logits with model\n",
    "y_logits = model(X_test.to(device))\n",
    "\n",
    "# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "print(y_logits[:5])\n",
    "print(y_pred_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 100 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 200 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 300 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 400 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 500 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 600 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 700 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 800 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 900 | Loss: 75.00000,| Test Loss: 2.40205\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = model(X_train) # model outputs raw logits \n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "  \n",
    "    loss = loss_fn(y_pred.to(torch.float32), y_train.squeeze()) \n",
    "  \n",
    "\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.requires_grad = True\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "      test_logits = model(X_test)\n",
    "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "      # 2. Calculate test loss and accuracy\n",
    "      test_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "\n",
    "    # Print out what's happening\n",
    "      if epoch % 100 == 0:\n",
    "          print(f\"Epoch: {epoch} | Loss: {loss:.5f},| Test Loss: {test_loss:.5f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
