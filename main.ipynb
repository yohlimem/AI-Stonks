{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mplfinance as mpf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"AAPL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open t - 1</th>\n",
       "      <th>Close t - 1</th>\n",
       "      <th>High t - 1</th>\n",
       "      <th>Low t - 1</th>\n",
       "      <th>...</th>\n",
       "      <th>Low t - 8</th>\n",
       "      <th>Open t - 9</th>\n",
       "      <th>Close t - 9</th>\n",
       "      <th>High t - 9</th>\n",
       "      <th>Low t - 9</th>\n",
       "      <th>Open t - 10</th>\n",
       "      <th>Close t - 10</th>\n",
       "      <th>High t - 10</th>\n",
       "      <th>Low t - 10</th>\n",
       "      <th>movement</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-07-24</th>\n",
       "      <td>224.000000</td>\n",
       "      <td>224.800003</td>\n",
       "      <td>217.130005</td>\n",
       "      <td>218.539993</td>\n",
       "      <td>218.539993</td>\n",
       "      <td>61777600</td>\n",
       "      <td>224.369995</td>\n",
       "      <td>225.009995</td>\n",
       "      <td>226.940002</td>\n",
       "      <td>222.679993</td>\n",
       "      <td>...</td>\n",
       "      <td>228.679993</td>\n",
       "      <td>231.389999</td>\n",
       "      <td>227.570007</td>\n",
       "      <td>232.389999</td>\n",
       "      <td>225.770004</td>\n",
       "      <td>229.300003</td>\n",
       "      <td>232.979996</td>\n",
       "      <td>233.080002</td>\n",
       "      <td>229.250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-25</th>\n",
       "      <td>218.929993</td>\n",
       "      <td>220.850006</td>\n",
       "      <td>214.619995</td>\n",
       "      <td>217.490005</td>\n",
       "      <td>217.490005</td>\n",
       "      <td>51391200</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>218.539993</td>\n",
       "      <td>224.800003</td>\n",
       "      <td>217.130005</td>\n",
       "      <td>...</td>\n",
       "      <td>233.089996</td>\n",
       "      <td>228.919998</td>\n",
       "      <td>230.539993</td>\n",
       "      <td>232.639999</td>\n",
       "      <td>228.679993</td>\n",
       "      <td>231.389999</td>\n",
       "      <td>227.570007</td>\n",
       "      <td>232.389999</td>\n",
       "      <td>225.770004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26</th>\n",
       "      <td>218.699997</td>\n",
       "      <td>219.490005</td>\n",
       "      <td>216.009995</td>\n",
       "      <td>217.960007</td>\n",
       "      <td>217.960007</td>\n",
       "      <td>41601300</td>\n",
       "      <td>218.929993</td>\n",
       "      <td>217.490005</td>\n",
       "      <td>220.850006</td>\n",
       "      <td>214.619995</td>\n",
       "      <td>...</td>\n",
       "      <td>232.330002</td>\n",
       "      <td>236.479996</td>\n",
       "      <td>234.399994</td>\n",
       "      <td>237.229996</td>\n",
       "      <td>233.089996</td>\n",
       "      <td>228.919998</td>\n",
       "      <td>230.539993</td>\n",
       "      <td>232.639999</td>\n",
       "      <td>228.679993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>216.960007</td>\n",
       "      <td>219.300003</td>\n",
       "      <td>215.750000</td>\n",
       "      <td>218.240005</td>\n",
       "      <td>218.240005</td>\n",
       "      <td>36311800</td>\n",
       "      <td>218.699997</td>\n",
       "      <td>217.960007</td>\n",
       "      <td>219.490005</td>\n",
       "      <td>216.009995</td>\n",
       "      <td>...</td>\n",
       "      <td>226.639999</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>234.820007</td>\n",
       "      <td>236.270004</td>\n",
       "      <td>232.330002</td>\n",
       "      <td>236.479996</td>\n",
       "      <td>234.399994</td>\n",
       "      <td>237.229996</td>\n",
       "      <td>233.089996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-30</th>\n",
       "      <td>219.190002</td>\n",
       "      <td>220.330002</td>\n",
       "      <td>216.119995</td>\n",
       "      <td>218.800003</td>\n",
       "      <td>218.800003</td>\n",
       "      <td>41643800</td>\n",
       "      <td>216.960007</td>\n",
       "      <td>218.240005</td>\n",
       "      <td>219.300003</td>\n",
       "      <td>215.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>222.270004</td>\n",
       "      <td>229.449997</td>\n",
       "      <td>228.880005</td>\n",
       "      <td>231.460007</td>\n",
       "      <td>226.639999</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>234.820007</td>\n",
       "      <td>236.270004</td>\n",
       "      <td>232.330002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2024-07-24  224.000000  224.800003  217.130005  218.539993  218.539993   \n",
       "2024-07-25  218.929993  220.850006  214.619995  217.490005  217.490005   \n",
       "2024-07-26  218.699997  219.490005  216.009995  217.960007  217.960007   \n",
       "2024-07-29  216.960007  219.300003  215.750000  218.240005  218.240005   \n",
       "2024-07-30  219.190002  220.330002  216.119995  218.800003  218.800003   \n",
       "\n",
       "              Volume  Open t - 1  Close t - 1  High t - 1   Low t - 1  ...  \\\n",
       "Date                                                                   ...   \n",
       "2024-07-24  61777600  224.369995   225.009995  226.940002  222.679993  ...   \n",
       "2024-07-25  51391200  224.000000   218.539993  224.800003  217.130005  ...   \n",
       "2024-07-26  41601300  218.929993   217.490005  220.850006  214.619995  ...   \n",
       "2024-07-29  36311800  218.699997   217.960007  219.490005  216.009995  ...   \n",
       "2024-07-30  41643800  216.960007   218.240005  219.300003  215.750000  ...   \n",
       "\n",
       "             Low t - 8  Open t - 9  Close t - 9  High t - 9   Low t - 9  \\\n",
       "Date                                                                      \n",
       "2024-07-24  228.679993  231.389999   227.570007  232.389999  225.770004   \n",
       "2024-07-25  233.089996  228.919998   230.539993  232.639999  228.679993   \n",
       "2024-07-26  232.330002  236.479996   234.399994  237.229996  233.089996   \n",
       "2024-07-29  226.639999  235.000000   234.820007  236.270004  232.330002   \n",
       "2024-07-30  222.270004  229.449997   228.880005  231.460007  226.639999   \n",
       "\n",
       "            Open t - 10  Close t - 10  High t - 10  Low t - 10  movement  \n",
       "Date                                                                      \n",
       "2024-07-24   229.300003    232.979996   233.080002  229.250000         0  \n",
       "2024-07-25   231.389999    227.570007   232.389999  225.770004         0  \n",
       "2024-07-26   228.919998    230.539993   232.639999  228.679993         0  \n",
       "2024-07-29   236.479996    234.399994   237.229996  233.089996         1  \n",
       "2024-07-30   235.000000    234.820007   236.270004  232.330002         0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(stock_name, period=\"5d\", interval=\"1m\",back_time=5):\n",
    "\n",
    "    recent_data = yf.download(stock_name, period=period, interval=interval)\n",
    "    data_200 = yf.download(stock_name, period=period, interval=interval).tail(len(recent_data) + 200)\n",
    "    data_200 = data_200.drop(columns=[\"Adj Close\"])\n",
    "\n",
    "    data_200.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "   # recent_data = data_200.tail(len(recent_data))\n",
    "\n",
    "    recent_data = recent_data.iloc[:-1]\n",
    "\n",
    "    for i in range(back_time):\n",
    "        recent_data[f\"Open t - {i+1}\"] = recent_data[\"Open\"].shift((i+1))\n",
    "        recent_data[f\"Close t - {i+1}\"] = recent_data[\"Close\"].shift((i+1))\n",
    "        recent_data[f\"High t - {i+1}\"] = recent_data[\"High\"].shift((i+1))\n",
    "        recent_data[f\"Low t - {i+1}\"] = recent_data[\"Low\"].shift((i+1))\n",
    "    recent_data = recent_data.dropna()\n",
    "\n",
    "    recent_data[\"movement\"] = (recent_data[\"Close\"] > recent_data[\"Open\"]).astype(int)\n",
    "    \n",
    "\n",
    "\n",
    "       \n",
    "    # add_plot = mpf.make_addplot(recent_data[\"200_day\"], color=\"blue\", linestyle=\"--\")\n",
    "\n",
    "    return recent_data\n",
    "\n",
    "recent_data = prepare_data(stock_name,back_time=10,period=\"1mo\",interval=\"1d\")#data\n",
    "\n",
    "recent_data.head()#sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "X = recent_data.drop([\"Close\", \"Volume\", \"High\", \"Low\",\"movement\"], axis=1)\n",
    "y = recent_data[[\"movement\"]]\n",
    "\n",
    "X_tensor = torch.from_numpy(X.values)\n",
    "y_tensor = torch.from_numpy(y.values)\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tensor,y_tensor,test_size=0.33)\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.to(torch.float32).to(device)\n",
    "# X_time_step = [i for i in range(len(X_train))]\n",
    "X_test= X_test.to(torch.float32).to(device)\n",
    "y_train = y_train.to(torch.float32).to(device)\n",
    "y_test =y_test.to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "\n",
    "        self.ltsm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "    def forward(self, sequences):\n",
    "        lstm_out, (hn, cn) = self.ltsm(sequences)\n",
    "        return lstm_out\n",
    "    \n",
    "model = nn.Sequential(\n",
    "    \n",
    "    LSTMPredictor(input_size=42, hidden_size=128, n_layers=10),\n",
    "    nn.Linear(128, 1),\n",
    "    \n",
    ").to(device)\n",
    "\n",
    "# model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[219.1500, 219.8600, 224.3700, 218.3600, 224.4800, 217.0200, 221.4400,\n",
       "         222.0800, 223.8200, 220.6300, 219.1900, 218.8000, 220.3300, 216.1200,\n",
       "         216.9600, 218.2400, 219.3000, 215.7500, 218.7000, 217.9600, 219.4900,\n",
       "         216.0100, 218.9300, 217.4900, 220.8500, 214.6200, 224.0000, 218.5400,\n",
       "         224.8000, 217.1300, 224.3700, 225.0100, 226.9400, 222.6800, 227.0100,\n",
       "         223.9600, 227.7800, 223.0900, 224.8200, 224.3100, 226.8000, 223.2800],\n",
       "        [224.0000, 218.5400, 224.3700, 225.0100, 226.9400, 222.6800, 227.0100,\n",
       "         223.9600, 227.7800, 223.0900, 224.8200, 224.3100, 226.8000, 223.2800,\n",
       "         230.2800, 224.1800, 230.4400, 222.2700, 229.4500, 228.8800, 231.4600,\n",
       "         226.6400, 235.0000, 234.8200, 236.2700, 232.3300, 236.4800, 234.4000,\n",
       "         237.2300, 233.0900, 228.9200, 230.5400, 232.6400, 228.6800, 231.3900,\n",
       "         227.5700, 232.3900, 225.7700, 229.3000, 232.9800, 233.0800, 229.2500],\n",
       "        [206.9000, 209.8200, 205.3000, 207.2300, 209.9900, 201.0700, 199.0900,\n",
       "         209.2700, 213.5000, 196.0000, 219.1500, 219.8600, 225.6000, 217.7100,\n",
       "         224.3700, 218.3600, 224.4800, 217.0200, 221.4400, 222.0800, 223.8200,\n",
       "         220.6300, 219.1900, 218.8000, 220.3300, 216.1200, 216.9600, 218.2400,\n",
       "         219.3000, 215.7500, 218.7000, 217.9600, 219.4900, 216.0100, 218.9300,\n",
       "         217.4900, 220.8500, 214.6200, 224.0000, 218.5400, 224.8000, 217.1300],\n",
       "        [219.1900, 218.8000, 216.9600, 218.2400, 219.3000, 215.7500, 218.7000,\n",
       "         217.9600, 219.4900, 216.0100, 218.9300, 217.4900, 220.8500, 214.6200,\n",
       "         224.0000, 218.5400, 224.8000, 217.1300, 224.3700, 225.0100, 226.9400,\n",
       "         222.6800, 227.0100, 223.9600, 227.7800, 223.0900, 224.8200, 224.3100,\n",
       "         226.8000, 223.2800, 230.2800, 224.1800, 230.4400, 222.2700, 229.4500,\n",
       "         228.8800, 231.4600, 226.6400, 235.0000, 234.8200, 236.2700, 232.3300],\n",
       "        [199.0900, 209.2700, 219.1500, 219.8600, 225.6000, 217.7100, 224.3700,\n",
       "         218.3600, 224.4800, 217.0200, 221.4400, 222.0800, 223.8200, 220.6300,\n",
       "         219.1900, 218.8000, 220.3300, 216.1200, 216.9600, 218.2400, 219.3000,\n",
       "         215.7500, 218.7000, 217.9600, 219.4900, 216.0100, 218.9300, 217.4900,\n",
       "         220.8500, 214.6200, 224.0000, 218.5400, 224.8000, 217.1300, 224.3700,\n",
       "         225.0100, 226.9400, 222.6800, 227.0100, 223.9600, 227.7800, 223.0900],\n",
       "        [216.9600, 218.2400, 218.7000, 217.9600, 219.4900, 216.0100, 218.9300,\n",
       "         217.4900, 220.8500, 214.6200, 224.0000, 218.5400, 224.8000, 217.1300,\n",
       "         224.3700, 225.0100, 226.9400, 222.6800, 227.0100, 223.9600, 227.7800,\n",
       "         223.0900, 224.8200, 224.3100, 226.8000, 223.2800, 230.2800, 224.1800,\n",
       "         230.4400, 222.2700, 229.4500, 228.8800, 231.4600, 226.6400, 235.0000,\n",
       "         234.8200, 236.2700, 232.3300, 236.4800, 234.4000, 237.2300, 233.0900],\n",
       "        [221.4400, 222.0800, 219.1900, 218.8000, 220.3300, 216.1200, 216.9600,\n",
       "         218.2400, 219.3000, 215.7500, 218.7000, 217.9600, 219.4900, 216.0100,\n",
       "         218.9300, 217.4900, 220.8500, 214.6200, 224.0000, 218.5400, 224.8000,\n",
       "         217.1300, 224.3700, 225.0100, 226.9400, 222.6800, 227.0100, 223.9600,\n",
       "         227.7800, 223.0900, 224.8200, 224.3100, 226.8000, 223.2800, 230.2800,\n",
       "         224.1800, 230.4400, 222.2700, 229.4500, 228.8800, 231.4600, 226.6400],\n",
       "        [213.1100, 213.3100, 206.9000, 209.8200, 213.6400, 206.3900, 205.3000,\n",
       "         207.2300, 209.9900, 201.0700, 199.0900, 209.2700, 213.5000, 196.0000,\n",
       "         219.1500, 219.8600, 225.6000, 217.7100, 224.3700, 218.3600, 224.4800,\n",
       "         217.0200, 221.4400, 222.0800, 223.8200, 220.6300, 219.1900, 218.8000,\n",
       "         220.3300, 216.1200, 216.9600, 218.2400, 219.3000, 215.7500, 218.7000,\n",
       "         217.9600, 219.4900, 216.0100, 218.9300, 217.4900, 220.8500, 214.6200]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss = 0.002593838609755039, test loss = 890050.75\n",
      "epoch: 100 loss = 0.0035256887786090374, test loss = 49.727272033691406\n",
      "epoch: 200 loss = 0.0010608789743855596, test loss = 1889.29541015625\n",
      "epoch: 300 loss = 0.0019466890953481197, test loss = 1353.6676025390625\n",
      "epoch: 400 loss = 0.003622602904215455, test loss = 14050883.0\n",
      "epoch: 500 loss = 0.0033486869651824236, test loss = 193.32086181640625\n",
      "epoch: 600 loss = 0.0031670168973505497, test loss = 4284.4970703125\n",
      "epoch: 700 loss = 0.0025408484507352114, test loss = 167.01861572265625\n",
      "epoch: 800 loss = 0.004147311206907034, test loss = 483.46160888671875\n",
      "epoch: 900 loss = 0.0020155487582087517, test loss = 52.639862060546875\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_test_pred = model(X_test)\n",
    "        test_loss = loss_fn(y_test_pred, y_test)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch: {epoch} loss = {loss}, test loss = {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_loop_data(stock_name, period=\"5d\", interval=\"1m\",back_time=5, answers =[]):\n",
    "\n",
    "    recent_data = yf.download(stock_name, period=period, interval=interval)\n",
    "\n",
    "    data_200 = yf.download(stock_name, period=period, interval=interval).tail(len(recent_data) + 201)\n",
    "    data_200 = data_200.drop(columns=[\"Adj Close\"])\n",
    "\n",
    "    data_200.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "    #recent_data = data_200.tail(len(recent_data))\n",
    "\n",
    "\n",
    "    recent_data = recent_data.dropna()\n",
    "    for i in range(back_time):\n",
    "        recent_data[f\"Open t - {i+1}\"] = recent_data[\"Open\"].shift((i+1))\n",
    "        recent_data[f\"Close t - {i+1}\"] = recent_data[\"Close\"].shift((i+1))\n",
    "        recent_data[f\"High t - {i+1}\"] = recent_data[\"High\"].shift((i+1))\n",
    "        recent_data[f\"Low t - {i+1}\"] = recent_data[\"Low\"].shift((i+1))\n",
    "    recent_data[\"movement\"] = (recent_data[\"Close\"] > recent_data[\"Open\"]).astype(int)\n",
    "\n",
    "    # recent_data = recent_data.tail(1)\n",
    "    # add_plot = mpf.make_addplot(recent_data[\"200_day\"], color=\"blue\", linestyle=\"--\")\n",
    "\n",
    "    return recent_data\n",
    "\n",
    "#recent_data = prepare_test_loop_data(stock_name,back_time=1,answers=[answer])\n",
    "#recent_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 46\u001b[0m\n\u001b[0;32m     40\u001b[0m answer \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(answer[\u001b[38;5;241m0\u001b[39m],check_X[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose t - 1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     41\u001b[0m answers\u001b[38;5;241m.\u001b[39mappend(answer)\n\u001b[0;32m     42\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     43\u001b[0m     {\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m\"\u001b[39m: [answers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m\"\u001b[39m: [answers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m---> 46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m: [answers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m]],\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m\"\u001b[39m: [answers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m3\u001b[39m]],\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdj Close\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     50\u001b[0m     },\n\u001b[0;32m     51\u001b[0m     index\u001b[38;5;241m=\u001b[39m[date_range[i]],\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     54\u001b[0m check_data \u001b[38;5;241m=\u001b[39m check_data\u001b[38;5;241m.\u001b[39m_append(df)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "answers = []\n",
    "forcast = 10\n",
    "check_data = prepare_test_loop_data(\n",
    "    interval=\"1m\", back_time=10, period=\"1d\", stock_name=stock_name\n",
    ")\n",
    "date_range = pd.date_range(\n",
    "    start=check_data.index[-1], periods=forcast, freq=\"1T\"\n",
    ")  # Generate a date range\n",
    "\n",
    "\n",
    "\n",
    "for i in range(forcast):\n",
    "    # for i in answers:\n",
    "\n",
    "    check_data_last = check_data.tail(1)\n",
    "    check_X = check_data_last.drop([\"Close\", \"Volume\", \"High\", \"Low\",\"movement\"], axis=1)\n",
    "    check_y = check_data_last[[\"movement\"]]\n",
    "\n",
    "    # print(check_data.tail(6).to_markdown())\n",
    "    # print(X.shape)\n",
    "\n",
    "    check_X_tensor = torch.from_numpy(check_X.values)\n",
    "    check_y_tensor = torch.from_numpy(check_y.values)\n",
    "\n",
    "    check_X_tensor = check_X_tensor.to(torch.float32).to(device)\n",
    "    check_y_tensor = check_y_tensor.to(torch.float32).to(device)\n",
    "\n",
    "    # print(check_data_last.to_markdown(), \"\\n\\n\\n\")\n",
    "\n",
    "    answer = model(check_X_tensor)\n",
    "    a = torch.softmax(answer, dim=1).argmax(dim=1)\n",
    "    print(a)\n",
    "\n",
    "    answer = answer.detach().cpu().numpy()\n",
    "\n",
    "    answer = np.append(answer[0],check_X[\"Close t - 1\"])\n",
    "    answers.append(answer)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Close\": [answers[-1][0]],\n",
    "            \"High\": [answers[-1][1]],\n",
    "            \"Low\": [answers[-1][2]],\n",
    "            \"Open\": [answers[-1][3]],\n",
    "            \"Adj Close\": [0],\n",
    "            \"Volume\": [0],\n",
    "        },\n",
    "        index=[date_range[i]],\n",
    "    )\n",
    "\n",
    "    check_data = check_data._append(df)\n",
    "    for i in range(10):\n",
    "        check_data[f\"Open t - {i+1}\"] = check_data[\"Open\"].shift((i + 1))\n",
    "        check_data[f\"Close t - {i+1}\"] = check_data[\"Close\"].shift((i + 1))\n",
    "        check_data[f\"High t - {i+1}\"] = check_data[\"High\"].shift((i + 1))\n",
    "        check_data[f\"Low t - {i+1}\"] = check_data[\"Low\"].shift((i + 1))\n",
    "    recent_data[\"movement\"] = (recent_data[\"Close\"] > recent_data[\"Open\"]).astype(int)\n",
    "\n",
    "    # check_data.dropna(inplace=True)\n",
    "# print(check_data.to_markdown())\n",
    "\n",
    "# print(check_data)\n",
    "# print(answers)\n",
    "\n",
    "# sns.catplot(answer[0][:],label=\"Predicted\")\n",
    "last_elements = [arr[0] for arr in answers]\n",
    "\n",
    "recent_data = prepare_data(stock_name, period=\"1d\", interval=\"1m\")\n",
    "# print(\"Converted index to datetime\")\n",
    "\n",
    "answers_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            recent_data,\n",
    "            columns=[\"Close\", \"High\", \"Low\", \"Open\",\"movement\"],\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            answers, columns=[\"movement\"], index=date_range\n",
    "        ),\n",
    "    ]\n",
    ").iloc[200:]\n",
    "\n",
    "# print(answers_df.tail(100))\n",
    "\n",
    "mpf.plot(\n",
    "    answers_df,\n",
    "    type=\"candle\",\n",
    "    style=\"charles\",\n",
    "    title=f\"{stock_name} Candlestick Chart\",\n",
    "    ylabel=\"Price\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 24\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Build model\n",
    "class stonks(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "             nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features), \n",
    "            nn.Sigmoid(),\n",
    "     \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "model = stonks(input_features=X_train.size()[1], \n",
    "                    output_features=1, \n",
    "                    hidden_units=8).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1976e-05],\n",
      "        [6.7180e-05],\n",
      "        [2.2683e-05],\n",
      "        [2.2327e-05]], grad_fn=<SliceBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Make prediction logits with model\n",
    "y_logits = model(X_test.to(device))\n",
    "\n",
    "# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "print(y_logits[:5])\n",
    "print(y_pred_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 100 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 200 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 300 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 400 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 500 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 600 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 700 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 800 | Loss: 75.00000,| Test Loss: 2.40205\n",
      "Epoch: 900 | Loss: 75.00000,| Test Loss: 2.40205\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = model(X_train) # model outputs raw logits \n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "  \n",
    "    loss = loss_fn(y_pred.to(torch.float32), y_train.squeeze()) \n",
    "  \n",
    "\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.requires_grad = True\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "      test_logits = model(X_test)\n",
    "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "      # 2. Calculate test loss and accuracy\n",
    "      test_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "\n",
    "    # Print out what's happening\n",
    "      if epoch % 100 == 0:\n",
    "          print(f\"Epoch: {epoch} | Loss: {loss:.5f},| Test Loss: {test_loss:.5f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
